{"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zbGBJegvBuXV"},"source":["# Inferring video resolution from encrypted traffic"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"qpuVcXRBB-3q"},"source":["In this exercise we will learn how to features extracted from encrypted traffic to infer the video resolution every 10 seconds of the session. The ultimate goal is to learn how different features impact differently the accuracy of inference model."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"e0qIjlyrCedM"},"source":["For this exercise, we will use dataset containing 4000 video sessions from four services: Netflix, YouTube, Twitch, and Amazon Prime Video. The trace is provided as a pandas DataFrame. For more information on pandas: https://pandas.pydata.org/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"nLZCOS_tAGV3"},"outputs":[],"source":["#Get the data\n","import os\n","if not os.path.exists(\"/data/video_dataset.pkl\"):\n","  !gdown https://drive.google.com/uc?id=1PHvEID7My6VZXZveCpQYy3lMo9RvMNTI -O data/video_dataset.pkl"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"EDDLJTyBBwux"},"source":["### Data cleaning"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"mM-vc0KyDB-a"},"source":["The first part of the model design pipeline requires you to explore the available dataset and remove possible bad or highly biased values."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"SXNgM_EhIOWu"},"outputs":[],"source":["import pandas as pd\n","\n","# Load the dataset\n","df = pd.read_pickle('/data/video_dataset.pkl')\n","\n","# Explore the structure of the dataset (e.g. the features that it contains)\n","df.head()"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"k4FLTfikIoH-"},"source":["Remove \"bad\" values"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"-HBfnklfInAy"},"outputs":[],"source":["# Tip: These are the only valid values for resolutions\n","valid_resolutions = [\n","                     280,\n","                     360,\n","                     480,\n","                     720,\n","                     1080\n","]\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"KnEsps5DIk5g"},"source":["Remove unwanted bias"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"4Wqrf8rVDhrn"},"source":["Sometimes datasets might contain features that could impact the accuracy of the model. Explore the dataset and remove the columns that you believe would have a negative effect on the final model due to unwanted bias."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"V_wmNNO6J63y"},"outputs":[],"source":["# Tip: strings are definitely a problem! For example:\n","unwanted_data = [\n","  \"video_id\",\n","  'home_id'\n","]\n","\n","# To drop the unwanted columns\n","df = df.drop(columns=unwanted_data)\n","\n","# What else?"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"TOPPsKpYB6PS"},"source":["### Simple quality inference\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Bso4_jhqEeZ3"},"source":["Let's try a first attempt at inferring the resolution.\n","\n","First, define the target of the inference.\n","Then prepare the data.\n","\n","Finally train and test your model. How is the performance?"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"VWqSCf3WOr1-"},"outputs":[],"source":["import numpy as np\n","\n","# Split the data in train / test datasets. What is the best unit to split your dataset?\n","\n","train_sessions = np.random.choice(df['session_id'].unique(), int(df['session_id'].unique().shape[0]*.8), replace=False)\n","test_sessions = df[~df['session_id'].isin(train_sessions)]['session_id'].unique()\n","\n","df_train = df[df['session_id'].isin(train_sessions)]\n","df_test = df[df['session_id'].isin(test_sessions)]\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"QAAACRWZKvPm"},"outputs":[],"source":["from matplotlib import pyplot as plt\n","import numpy as np\n","import pandas as pd\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.preprocessing import label_binarize\n","from sklearn import linear_model, ensemble, tree, model_selection, metrics\n","from sklearn.metrics import roc_curve, auc, average_precision_score, precision_recall_curve\n","from sklearn.multiclass import OneVsRestClassifier\n","\n","x_train = df_train.drop(['resolution', 'session_id', 'service'], axis=1).values\n","x_test = df_test.drop(['resolution', 'session_id', 'service'], axis=1).values\n","\n","classes = [\n","      240,\n","      360,\n","      480,\n","      720,\n","      1080\n","]\n","\n","y_train_not_binarized = df_train['resolution'].values\n","y_train_binarized = label_binarize(y_train_not_binarized, classes=classes)\n","y_test_not_binarized = df_test['resolution']\n","y_test_binarized = label_binarize(y_test_not_binarized, classes=classes)\n","\n","# Compute ROC curve and ROC area for each class\n","fpr = dict()\n","tpr = dict()\n","roc_auc = dict()\n","\n","# Learn to predict each class against the other\n","classifier = OneVsRestClassifier(RandomForestClassifier())\n","\n","gen_clf = classifier.fit(x_train, y_train_binarized)\n","\n","y_score = gen_clf.predict_proba(x_test)\n","\n","n_classes = y_test_binarized.shape[1]\n","# Get ROC curve per-class\n","for i in range(0, n_classes):\n","  fpr[i], tpr[i], _ = roc_curve(y_test_binarized[:, i], y_score[:, i])\n","  roc_auc[i] = auc(fpr[i], tpr[i])\n","\n","#Compute the Micro-average across classes\n","fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test_binarized.ravel(), y_score.ravel())\n","roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n","\n","# Get ROC curve\n","precision = dict()\n","recall = dict()\n","average_precision = dict()\n","\n","for i in range(n_classes):\n","  precision[i], recall[i], _ = precision_recall_curve(y_test_binarized[:, i], y_score[:, i])\n","  average_precision[i] = average_precision_score(y_test_binarized[:, i], y_score[:, i])\n","\n","# A \"micro-average\": quantifying score on all classes jointly\n","precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(y_test_binarized.ravel(), y_score.ravel())\n","average_precision[\"micro\"] = average_precision_score(y_test_binarized, y_score, average=\"micro\")\n","\n","plt.plot(fpr[\"micro\"], tpr[\"micro\"], label='Mean ROC (AUC = %0.2f)' % (roc_auc[\"micro\"]), \n","         linestyle='-', linewidth=0.8, marker='*', markersize=3)\n","plt.xlabel('False Postives Rate')\n","plt.ylabel('True Postives Rate')\n","plt.grid(True, which='major', axis='both')\n","plt.legend()\n","plt.show()\n","\n","plt.plot(recall[\"micro\"], precision[\"micro\"], label=' P/R curve (AP = %0.2f)' % (average_precision[\"micro\"]), \n","         linestyle='-', linewidth=0.8, marker='*', markersize=3)\n","plt.xlabel('Recall')\n","plt.ylabel('Precision')\n","plt.legend()\n","plt.grid(True, which='major', axis='both')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"0qFV6q2OCCsK"},"source":["### Feature importance"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"IBsSRRiqIA-v"},"source":["After designing your first model, it's important to understand which features had the highest impact on your prediction accuracy.\n","\n","REMEMBER: extracting features from network traffic is costly! The more features you use the more powerful your network capturing tools have to be.\n","\n","In this exercise we want to quantitatively study which features yield the highest inference power. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"2jkzqGQOQGtf"},"outputs":[],"source":["# Collect in an ordered array the features and their importance in the prediction\n","feature_importance = []\n","\n","# Here is an example of how to get the feature importance for \n","for i, feature in enumerate(features):\n","  feature_importance.append({'name': feature, 'GINI_index': clf.estimators_[1].feature_importances_[i]})\n","\n","feature_importance = sorted(feature_importance, key=lambda k: k['GINI_index'], reverse=True)\n","\n","# Which are the most important features?"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"lBCP_2SBC2xj"},"source":["### Select features by layer\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zKg-q0qSjzlU"},"source":["In the previous exercise you studied which features have the highest impact on the inference accuracy. We use the otained results to group features into groups and evaluate which collection of features achieves the highest accuracy.\n","\n","Remember that features from the same layer might be using the same information to be computed!"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"5lFpRXNkSv3i"},"outputs":[],"source":["# Hint: features are conveniently tagged with the layer they belong to. E.g.:\n","l3_features = [col for col in df.columns if 'L3' in col]\n","\n","# Replicate the study from the previous exercise (\"Simple quality inference\")\n","# using different feature groups.\n","\n","# What do you observe?\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"uoeohDnlwTLd"},"source":["# Predict the ongoing resolution of a real Netflix session"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"CiG-HjdUwUrg"},"source":["Now that you have your model, it's time to put it in practice!\n","\n","Use a preprocessed Netflix video session to infer the resolution at 10 second time windows"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"u2kV-SL3wbFD"},"outputs":[],"source":["#Get the data\n","if not os.path.exists(\"/data/netflix_session.pkl\"):\n","  !gdown https://drive.google.com/uc?id=1N-Cf4dJ3fpak_AWgO05Fopq_XPYLVqdS -O data/netflix_session.pkl"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"rUF2s6Es17Oq"},"outputs":[],"source":["df_session = pd.read_pickle(\"netflix_session.pkl\")\n","\n","unwanted_data = [\n","  \"video_id\",\n","  \"video_position\",\n","  \"index\",\n","  'home_id',\n","  \"relative_timestamp\",\n","  \"absolute_timestamp\",\n","  'resolution', \n","  'session_id'\n","]\n","\n","x = df_session.drop(columns=unwanted_data).values\n","y = [0 if v is None else int(v) for v in df_session['resolution'].values]\n","\n","# Predict the inferred resolutions and compare\n","\n","# You can use this code to plot the result (predicted_resolutions is a list)\n","plt.plot(df_session['relative_timestamp'].values, y, label='Real')\n","plt.plot(df_session['relative_timestamp'].values, predicted_resolutions, label='Predicted')\n","plt.xlabel('Session time')\n","plt.ylabel('Resolution')\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Which layer is the most effective in the prediction?\n","\n","Answer here"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNl03paGy+OiJQlpQTQPnoF","collapsed_sections":[],"name":"feature_importance.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}
